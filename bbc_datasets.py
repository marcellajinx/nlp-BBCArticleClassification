# -*- coding: utf-8 -*-
"""bbc_datasets

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aHz5ZAmrX-raQ_pnUtxz8ao4kOHl4dkf

Marcella Komunita Pasaribu <br>
https://www.kaggle.com/yufengdev/bbc-fulltext-and-category/code
"""

import pandas as pd
import tensorflow as tf

stopwords = [ "a", "about", "above", "after", "again", "against", "all", "am", "an", "and", "any", "are", "as", "at", "be", "because", "been", "before", 
             "being", "below", "between", "both", "but", "by", "could", "did", "do", "does", "doing", "down", "during", "each", "few", "for", "from", 
             "further", "had", "has", "have", "having", "he", "he'd", "he'll", "he's", "her", "here", "here's", "hers", "herself", "him", "himself", 
             "his", "how", "how's", "i", "i'd", "i'll", "i'm", "i've", "if", "in", "into", "is", "it", "it's", "its", "itself", "let's", "me", "more", 
             "most", "my", "myself", "nor", "of", "on", "once", "only", "or", "other", "ought", "our", "ours", "ourselves", "out", "over", "own", "same", 
             "she", "she'd", "she'll", "she's", "should", "so", "some", "such", "than", "that", "that's", "the", "their", "theirs", "them", "themselves", 
             "then", "there", "there's", "these", "they", "they'd", "they'll", "they're", "they've", "this", "those", "through", "to", "too", "under", 
             "until", "up", "very", "was", "we", "we'd", "we'll", "we're", "we've", "were", "what", "what's", "when", "when's", "where", "where's", "which", 
             "while", "who", "who's", "whom", "why", "why's", "with", "would", "you", "you'd", "you'll", "you're", "you've", "your", "yours", "yourself", 
             "yourselves" ]

def rm_stop_words(str):
  final_text = []
  for i in str.split():
      if i.strip().lower() not in stopwords:
        final_text.append(i.strip())
  return ' '.join(final_text)

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/My Drive/bbc-text.csv')
df

category = pd.get_dummies(df.category)
df_baru = pd.concat([df, category], axis=1)
df_baru = df_baru.drop(columns='category')
df_baru['text'] = df_baru['text'].apply(rm_stop_words)
df_baru

teks = df_baru['text']
label = df_baru[['business', 'entertainment', 'politics',	'sport',	'tech']].values

from sklearn.model_selection import train_test_split
teks_latih, teks_test, label_latih, label_test = train_test_split(teks, label, test_size=0.2, shuffle=True)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
 
tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(teks_latih)
 
sekuens_latih = tokenizer.texts_to_sequences(teks_latih)
sekuens_test = tokenizer.texts_to_sequences(teks_test)
 
padded_latih = pad_sequences(sekuens_latih, maxlen=120) 
padded_test = pad_sequences(sekuens_test, maxlen=120)

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.92 and logs.get('val_accuracy')>0.92):
      self.model.stop_training = True
      print("\nAkurasi > 92%")
callbacks = myCallback()

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(5000, 64, input_length=120),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(5, activation='softmax')
])
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

hist = model.fit(padded_latih, label_latih, epochs=30, 
                    validation_data=(padded_test, label_test), 
                    verbose=2, 
                    callbacks=[callbacks])

import matplotlib.pyplot as plt

#plot akurasi
plt.plot(hist.history['accuracy'], label='Training')
plt.plot(hist.history['val_accuracy'], label='Validasi')

plt.title('Plot Akurasi')
plt.ylabel('Akurasi')
plt.xlabel('Epoch')
plt.legend()

plt.show()

#plot loss
plt.plot(hist.history['loss'], label='Training')
plt.plot(hist.history['val_loss'], label='Validasi')

plt.title('Plot Loss')
plt.ylabel('Akurasi')
plt.xlabel('Epoch')
plt.legend()

plt.show()

